{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7FfawJwDFDC",
        "outputId": "1e86cdf0-5ae0-4ee8-bb4f-4970f6a0f322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Segment-Everything-Everywhere-All-At-Once'...\n",
            "remote: Enumerating objects: 1127, done.\u001b[K\n",
            "remote: Counting objects: 100% (357/357), done.\u001b[K\n",
            "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
            "remote: Total 1127 (delta 297), reused 278 (delta 278), pack-reused 770 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1127/1127), 303.82 MiB | 23.71 MiB/s, done.\n",
            "Resolving deltas: 100% (492/492), done.\n"
          ]
        }
      ],
      "source": [
        "#git clone\n",
        "\n",
        "!git clone https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요 패키지 설치\n",
        "\n",
        "!sudo apt update\n",
        "!sudo apt install ffmpeg\n",
        "!pip install -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt\n",
        "!pip install -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJkK5p0FDJ6S",
        "outputId": "fe2fbd7b-8084-4897-cd15-c5509c2a4653"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "25 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "Collecting torch==2.1.0 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1))\n",
            "  Using cached torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchvision==0.16.0 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 2))\n",
            "  Using cached torchvision-0.16.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting pillow==9.4.0 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 3))\n",
            "  Using cached Pillow-9.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting opencv-python==4.8.1.78 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 4))\n",
            "  Using cached opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting pyyaml==6.0.1 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 5))\n",
            "  Using cached PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting json_tricks==3.17.3 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 6))\n",
            "  Using cached json_tricks-3.17.3-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: yacs==0.1.8 in /usr/local/lib/python3.11/dist-packages (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 7)) (0.1.8)\n",
            "Collecting scikit-learn==1.3.1 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 8))\n",
            "  Using cached scikit_learn-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting pandas==2.0.3 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 9))\n",
            "  Using cached pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting timm==0.4.12 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 10))\n",
            "  Using cached timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting numpy==1.23.1 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 11))\n",
            "  Using cached numpy-1.23.1.tar.gz (10.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting einops==0.7.0 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 12))\n",
            "  Using cached einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: fvcore==0.1.5.post20221221 in /usr/local/lib/python3.11/dist-packages (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 13)) (0.1.5.post20221221)\n",
            "Collecting transformers==4.34.0 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 14))\n",
            "  Using cached transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
            "Collecting sentencepiece==0.1.99 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 15))\n",
            "  Using cached sentencepiece-0.1.99-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting ftfy==6.1.1 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 16))\n",
            "  Using cached ftfy-6.1.1-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting regex==2023.10.3 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 17))\n",
            "  Using cached regex-2023.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting nltk==3.8.1 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 18))\n",
            "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting mpi4py==3.1.5 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 19))\n",
            "  Using cached mpi4py-3.1.5.tar.gz (2.5 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting vision-datasets==0.2.2 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 20))\n",
            "  Using cached vision_datasets-0.2.2-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting cython==3.0.2 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 21))\n",
            "  Using cached Cython-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
            "Collecting pycocotools==2.0.7 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 22))\n",
            "  Using cached pycocotools-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting diffdist==0.1 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 23))\n",
            "  Using cached diffdist-0.1.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyarrow==13.0.0 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 24))\n",
            "  Using cached pyarrow-13.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting cityscapesscripts==2.2.2 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 25))\n",
            "  Using cached cityscapesScripts-2.2.2-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting shapely==1.8.0 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 26))\n",
            "  Using cached Shapely-1.8.0.tar.gz (278 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-image==0.21.0 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 27))\n",
            "  Using cached scikit_image-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting mup==1.0.0 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 28))\n",
            "  Using cached mup-1.0.0.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate==0.23.0 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 29))\n",
            "  Using cached accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting kornia==0.7.0 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 30))\n",
            "  Using cached kornia-0.7.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting deepspeed==0.10.3 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 31))\n",
            "  Using cached deepspeed-0.10.3.tar.gz (867 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb==0.15.12 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 32))\n",
            "  Using cached wandb-0.15.12-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting infinibatch==0.1.1 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 33))\n",
            "  Using cached infinibatch-0.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting gradio==3.42.0 (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 34))\n",
            "  Using cached gradio-3.42.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1)) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1)) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1)) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 1))\n",
            "  Using cached triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 2)) (2.32.3)\n",
            "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 2), -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements.txt (line 4) and numpy==1.23.1 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested numpy==1.23.1\n",
            "    torchvision 0.16.0 depends on numpy\n",
            "    opencv-python 4.8.1.78 depends on numpy>=1.21.2; python_version >= \"3.10\"\n",
            "    opencv-python 4.8.1.78 depends on numpy>=1.23.5; python_version >= \"3.11\"\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting git+https://github.com/arogozhnikov/einops.git (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 1))\n",
            "  Cloning https://github.com/arogozhnikov/einops.git to /tmp/pip-req-build-ge955_34\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/arogozhnikov/einops.git /tmp/pip-req-build-ge955_34\n",
            "  Resolved https://github.com/arogozhnikov/einops.git to commit 2d12427a70b673c493044a8a7acfbb4719537e00\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/MaureenZOU/detectron2-xyz.git (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2))\n",
            "  Cloning https://github.com/MaureenZOU/detectron2-xyz.git to /tmp/pip-req-build-zl45vpje\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MaureenZOU/detectron2-xyz.git /tmp/pip-req-build-zl45vpje\n",
            "  Resolved https://github.com/MaureenZOU/detectron2-xyz.git to commit 42121d75e10d9f858f3a91b6a39f5722c02868f0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/openai/whisper.git (from -r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3))\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-utk_809q\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-utk_809q\n",
            "  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (11.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (2.18.0)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (0.1.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: omegaconf>=2.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: black==21.4b2 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (21.4b2)\n",
            "Requirement already satisfied: scipy>1.5.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from black==21.4b2->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (8.1.8)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.11/dist-packages (from black==21.4b2->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (1.4.4)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from black==21.4b2->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (0.10.2)\n",
            "Requirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.11/dist-packages (from black==21.4b2->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: pathspec<1,>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from black==21.4b2->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black==21.4b2->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (10.6.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (0.61.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (2.5.1+cu124)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.1->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.1->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (3.17.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (0.44.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (4.25.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 3)) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6->-r /content/Segment-Everything-Everywhere-All-At-Once/assets/requirements/requirements_custom.txt (line 2)) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 추가 패키지 설정\n",
        "\n",
        "!pip install gradio==3.37.0\n",
        "!pip install pillow==9.5.0\n",
        "!pip install mpi4py\n",
        "!pip install kornia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYxbvbpODQHW",
        "outputId": "0e20d4ab-ea63-4bd5-9da4-dda56ad80112"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.16.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.8)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.7.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (9.5.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.9.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.45.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: pillow==9.5.0 in /usr/local/lib/python3.11/dist-packages (9.5.0)\n",
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.0.3.tar.gz (466 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.3-cp311-cp311-linux_x86_64.whl size=4458239 sha256=5c0b7292f37e49628c159cc999fa3be771ec3859f6948c404ed22e5c6c58c2b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/56/17/bf6ba37aa971a191a8b9eaa188bf5ec855b8911c1c56fb1f84\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.3\n",
            "Collecting kornia\n",
            "  Downloading kornia-0.8.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting kornia_rs>=0.1.0 (from kornia)\n",
            "  Downloading kornia_rs-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kornia) (24.2)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from kornia) (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.1->kornia) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.1->kornia) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.1->kornia) (2.1.5)\n",
            "Downloading kornia-0.8.0-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kornia_rs, kornia\n",
            "Successfully installed kornia-0.8.0 kornia_rs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "오류 수정\n",
        "\n",
        "demo/seem/app.py 수정\n",
        "\n",
        "31 lines : yaml 파일 경로 수정  \n",
        "    default=\"configs/seem/focall_unicl_lang_demo.yaml\"-> \"/content/Segment-Everything-Everywhere-All-At-Once/configs/seem/focall_unicl_lang_demo.yaml\"\n"
      ],
      "metadata": {
        "id": "gndAc_QhELMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "143~149 line 경로 수정\n",
        "demo -> /content/Segment-Everything-Everywhere-All-At-Once/demo"
      ],
      "metadata": {
        "id": "N4MT5SlkJrJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export PYTHONPATH=$PYTHONPATH:/content/Segment-Everything-Everywhere-All-At-Once && python /content/Segment-Everything-Everywhere-All-At-Once/demo/seem/app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7zMUtLPQQmV",
        "outputId": "c2e7a530-188e-4910-ad29-636577430580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio_client/documentation.py:106: UserWarning: Could not get documentation group for <class 'gradio.mix.Parallel'>: No known documentation group for module 'gradio.mix'\n",
            "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n",
            "/usr/local/lib/python3.11/dist-packages/gradio_client/documentation.py:106: UserWarning: Could not get documentation group for <class 'gradio.mix.Series'>: No known documentation group for module 'gradio.mix'\n",
            "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "Deformable Transformer Encoder is not available.\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "/content/Segment-Everything-Everywhere-All-At-Once/modeling/BaseModel.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(load_dir, map_location=self.opt['device'])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.gamma_1, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.gamma_2, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.f.bias, Model Shape: torch.Size([389]) <-> Ckpt Shape: torch.Size([389])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.f.weight, Model Shape: torch.Size([389, 192]) <-> Ckpt Shape: torch.Size([389, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([192, 1, 5, 5]) <-> Ckpt Shape: torch.Size([192, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([192, 1, 7, 7]) <-> Ckpt Shape: torch.Size([192, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([192, 1, 9, 9]) <-> Ckpt Shape: torch.Size([192, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.h.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.h.weight, Model Shape: torch.Size([192, 192, 1, 1]) <-> Ckpt Shape: torch.Size([192, 192, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.gamma_1, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.gamma_2, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.f.bias, Model Shape: torch.Size([389]) <-> Ckpt Shape: torch.Size([389])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.f.weight, Model Shape: torch.Size([389, 192]) <-> Ckpt Shape: torch.Size([389, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([192, 1, 5, 5]) <-> Ckpt Shape: torch.Size([192, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([192, 1, 7, 7]) <-> Ckpt Shape: torch.Size([192, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([192, 1, 9, 9]) <-> Ckpt Shape: torch.Size([192, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.h.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.h.weight, Model Shape: torch.Size([192, 192, 1, 1]) <-> Ckpt Shape: torch.Size([192, 192, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.downsample.norm.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.0.downsample.norm.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.0.downsample.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.0.downsample.proj.weight, Model Shape: torch.Size([384, 192, 3, 3]) <-> Ckpt Shape: torch.Size([384, 192, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.f.bias, Model Shape: torch.Size([773]) <-> Ckpt Shape: torch.Size([773])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.f.weight, Model Shape: torch.Size([773, 384]) <-> Ckpt Shape: torch.Size([773, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([384, 1, 9, 9]) <-> Ckpt Shape: torch.Size([384, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.f.bias, Model Shape: torch.Size([773]) <-> Ckpt Shape: torch.Size([773])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.f.weight, Model Shape: torch.Size([773, 384]) <-> Ckpt Shape: torch.Size([773, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([384, 1, 9, 9]) <-> Ckpt Shape: torch.Size([384, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.downsample.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.1.downsample.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.1.downsample.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.1.downsample.proj.weight, Model Shape: torch.Size([768, 384, 3, 3]) <-> Ckpt Shape: torch.Size([768, 384, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.downsample.norm.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.2.downsample.norm.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.2.downsample.proj.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.2.downsample.proj.weight, Model Shape: torch.Size([1536, 768, 3, 3]) <-> Ckpt Shape: torch.Size([1536, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.gamma_1, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.gamma_2, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([6144]) <-> Ckpt Shape: torch.Size([6144])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([6144, 1536]) <-> Ckpt Shape: torch.Size([6144, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([1536, 6144]) <-> Ckpt Shape: torch.Size([1536, 6144])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.f.bias, Model Shape: torch.Size([3077]) <-> Ckpt Shape: torch.Size([3077])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.f.weight, Model Shape: torch.Size([3077, 1536]) <-> Ckpt Shape: torch.Size([3077, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([1536, 1, 3, 3]) <-> Ckpt Shape: torch.Size([1536, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([1536, 1, 5, 5]) <-> Ckpt Shape: torch.Size([1536, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([1536, 1, 7, 7]) <-> Ckpt Shape: torch.Size([1536, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([1536, 1, 9, 9]) <-> Ckpt Shape: torch.Size([1536, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.h.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.h.weight, Model Shape: torch.Size([1536, 1536, 1, 1]) <-> Ckpt Shape: torch.Size([1536, 1536, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.proj.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.proj.weight, Model Shape: torch.Size([1536, 1536]) <-> Ckpt Shape: torch.Size([1536, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm1.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm2.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm2.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.gamma_1, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.gamma_2, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([6144]) <-> Ckpt Shape: torch.Size([6144])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([6144, 1536]) <-> Ckpt Shape: torch.Size([6144, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([1536, 6144]) <-> Ckpt Shape: torch.Size([1536, 6144])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.f.bias, Model Shape: torch.Size([3077]) <-> Ckpt Shape: torch.Size([3077])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.f.weight, Model Shape: torch.Size([3077, 1536]) <-> Ckpt Shape: torch.Size([3077, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([1536, 1, 3, 3]) <-> Ckpt Shape: torch.Size([1536, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([1536, 1, 5, 5]) <-> Ckpt Shape: torch.Size([1536, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([1536, 1, 7, 7]) <-> Ckpt Shape: torch.Size([1536, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([1536, 1, 9, 9]) <-> Ckpt Shape: torch.Size([1536, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.h.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.h.weight, Model Shape: torch.Size([1536, 1536, 1, 1]) <-> Ckpt Shape: torch.Size([1536, 1536, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.proj.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.proj.weight, Model Shape: torch.Size([1536, 1536]) <-> Ckpt Shape: torch.Size([1536, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm1.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm2.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm2.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.norm0.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.norm0.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.norm3.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.norm3.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.patch_embed.norm.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.patch_embed.norm.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.patch_embed.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.patch_embed.proj.weight, Model Shape: torch.Size([192, 3, 7, 7]) <-> Ckpt Shape: torch.Size([192, 3, 7, 7])\n",
            "INFO:utils.model:Loaded dilation_kernel, Model Shape: torch.Size([1, 1, 3, 3]) <-> Ckpt Shape: torch.Size([1, 1, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_1.weight, Model Shape: torch.Size([512, 192, 1, 1]) <-> Ckpt Shape: torch.Size([512, 192, 1, 1])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_2.weight, Model Shape: torch.Size([512, 384, 1, 1]) <-> Ckpt Shape: torch.Size([512, 384, 1, 1])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_3.weight, Model Shape: torch.Size([512, 768, 1, 1]) <-> Ckpt Shape: torch.Size([512, 768, 1, 1])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.input_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.input_proj.weight, Model Shape: torch.Size([512, 1536, 1, 1]) <-> Ckpt Shape: torch.Size([512, 1536, 1, 1])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_1.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_2.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_3.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_4.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.mask_features.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.mask_features.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.class_embed, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.decoder_norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.decoder_norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.positional_embedding, Model Shape: torch.Size([77, 512]) <-> Ckpt Shape: torch.Size([77, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.token_embedding.weight, Model Shape: torch.Size([49408, 512]) <-> Ckpt Shape: torch.Size([49408, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_proj, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.logit_scale, Model Shape: torch.Size([]) <-> Ckpt Shape: torch.Size([])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.level_embed.weight, Model Shape: torch.Size([3, 512]) <-> Ckpt Shape: torch.Size([3, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.0.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.0.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.1.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.2.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_sptial_embed.0, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_sptial_embed.1, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_sptial_embed.2, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.pn_indicator.weight, Model Shape: torch.Size([2, 512]) <-> Ckpt Shape: torch.Size([2, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.query_embed.weight, Model Shape: torch.Size([101, 512]) <-> Ckpt Shape: torch.Size([101, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.query_feat.weight, Model Shape: torch.Size([101, 512]) <-> Ckpt Shape: torch.Size([101, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.spatial_embed.weight, Model Shape: torch.Size([32, 512]) <-> Ckpt Shape: torch.Size([32, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.spatial_featured.weight, Model Shape: torch.Size([32, 512]) <-> Ckpt Shape: torch.Size([32, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "WARNING:utils.model:$UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([134])\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/content/Segment-Everything-Everywhere-All-At-Once/demo/seem/app.py:131: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  inputs = [ImageMask(label=\"[Stroke] Draw on Image\",type=\"pil\"), gr.inputs.CheckboxGroup(choices=[\"Stroke\", \"Example\", \"Text\", \"Audio\", \"Video\", \"Panoptic\"], type=\"value\", label=\"Interative Mode\"), ImageMask(label=\"[Example] Draw on Referring Image\",type=\"pil\"), gr.Textbox(label=\"[Text] Referring Text\"), gr.Audio(label=\"[Audio] Referring Audio\", source=\"microphone\", type=\"filepath\"), gr.Video(label=\"[Video] Referring Video Segmentation\",format=\"mp4\",interactive=True)]\n",
            "/content/Segment-Everything-Everywhere-All-At-Once/demo/seem/app.py:131: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  inputs = [ImageMask(label=\"[Stroke] Draw on Image\",type=\"pil\"), gr.inputs.CheckboxGroup(choices=[\"Stroke\", \"Example\", \"Text\", \"Audio\", \"Video\", \"Panoptic\"], type=\"value\", label=\"Interative Mode\"), ImageMask(label=\"[Example] Draw on Referring Image\",type=\"pil\"), gr.Textbox(label=\"[Text] Referring Text\"), gr.Audio(label=\"[Audio] Referring Audio\", source=\"microphone\", type=\"filepath\"), gr.Video(label=\"[Video] Referring Video Segmentation\",format=\"mp4\",interactive=True)]\n",
            "/content/Segment-Everything-Everywhere-All-At-Once/demo/seem/app.py:136: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  gr.outputs.Image(\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "IMPORTANT: You are using gradio version 3.37.0, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://78b9cc1bad9d4d5c66.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "/content/Segment-Everything-Everywhere-All-At-Once/utils/visualizer.py:218: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  yield (self._seg == sid).numpy().astype(np.bool), sinfo\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/routes.py\", line 439, in run_predict\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1389, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1094, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 704, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Segment-Everything-Everywhere-All-At-Once/demo/seem/app.py\", line 73, in inference\n",
            "    return interactive_infer_image(model, audio, image, task, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Segment-Everything-Everywhere-All-At-Once/demo/seem/tasks/interactive.py\", line 113, in interactive_infer_image\n",
            "    demo = visual.draw_panoptic_seg(pano_seg.cpu(), pano_seg_info) # rgb Image\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Segment-Everything-Everywhere-All-At-Once/utils/visualizer.py\", line 505, in draw_panoptic_seg\n",
            "    for mask, sinfo in pred.semantic_masks():\n",
            "  File \"/content/Segment-Everything-Everywhere-All-At-Once/utils/visualizer.py\", line 218, in semantic_masks\n",
            "    yield (self._seg == sid).numpy().astype(np.bool), sinfo\n",
            "                                            ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\", line 324, in __getattr__\n",
            "    raise AttributeError(__former_attrs__[attr])\n",
            "AttributeError: module 'numpy' has no attribute 'bool'.\n",
            "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
            "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/tmp/gradio/52e92c9b5989e6246d1a3eb128b8e1735c57a96a/vasedeck.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf57.83.100\n",
            "  Duration: 00:00:11.60, start: 0.000000, bitrate: 15515 kb/s\n",
            "  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 1344x1008 [SAR 1:1 DAR 4:3], 15514 kb/s, 10 fps, 10 tbr, 10240 tbn, 20 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      vendor_id       : [0][0][0][0]\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> png (native))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, image2, to '/tmp/gradio/52e92c9b5989e6246d1a3eb128b8e1735c57a96a/vasedeck/%04d.png':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.76.100\n",
            "  Stream #0:0(und): Video: png, rgb24(pc, gbr/unknown/unknown, progressive), 1344x1008 [SAR 1:1 DAR 4:3], q=2-31, 200 kb/s, 5 fps, 5 tbn (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      vendor_id       : [0][0][0][0]\n",
            "      encoder         : Lavc58.134.100 png\n",
            "frame=   58 fps=5.7 q=-0.0 Lsize=N/A time=00:00:11.60 bitrate=N/A speed=1.15x    \n",
            "video:130742kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/routes.py\", line 439, in run_predict\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1389, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1094, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 704, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Segment-Everything-Everywhere-All-At-Once/demo/seem/app.py\", line 71, in inference\n",
            "    return interactive_infer_video(model, audio, image, task, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Segment-Everything-Everywhere-All-At-Once/demo/seem/tasks/interactive.py\", line 211, in interactive_infer_video\n",
            "    refimg_ori, refimg_mask = refimg['image'], refimg['mask']\n",
            "                              ~~~~~~^^^^^^^^^\n",
            "TypeError: 'NoneType' object is not subscriptable\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/routes.py\", line 439, in run_predict\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1389, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1094, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 704, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Segment-Everything-Everywhere-All-At-Once/demo/seem/app.py\", line 71, in inference\n",
            "    return interactive_infer_video(model, audio, image, task, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Segment-Everything-Everywhere-All-At-Once/demo/seem/tasks/interactive.py\", line 189, in interactive_infer_video\n",
            "    input_dir = video_pth.replace('.mp4', '')\n",
            "                ^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'replace'\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n",
            "/content/Segment-Everything-Everywhere-All-At-Once/utils/visualizer.py:218: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  yield (self._seg == sid).numpy().astype(np.bool), sinfo\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/routes.py\", line 439, in run_predict\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1389, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1094, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 704, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Segment-Everything-Everywhere-All-At-Once/demo/seem/app.py\", line 73, in inference\n",
            "    return interactive_infer_image(model, audio, image, task, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Segment-Everything-Everywhere-All-At-Once/demo/seem/tasks/interactive.py\", line 113, in interactive_infer_image\n",
            "    demo = visual.draw_panoptic_seg(pano_seg.cpu(), pano_seg_info) # rgb Image\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Segment-Everything-Everywhere-All-At-Once/utils/visualizer.py\", line 505, in draw_panoptic_seg\n",
            "    for mask, sinfo in pred.semantic_masks():\n",
            "  File \"/content/Segment-Everything-Everywhere-All-At-Once/utils/visualizer.py\", line 218, in semantic_masks\n",
            "    yield (self._seg == sid).numpy().astype(np.bool), sinfo\n",
            "                                            ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\", line 324, in __getattr__\n",
            "    raise AttributeError(__former_attrs__[attr])\n",
            "AttributeError: module 'numpy' has no attribute 'bool'.\n",
            "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
            "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n"
          ]
        }
      ]
    }
  ]
}